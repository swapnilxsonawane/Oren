{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilxsonawane/Oren/blob/master/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "source": [
        "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "- <img src=\"/img/new.png\" height=\"20px\" align=\"left\" hspace=\"4px\" alt=\"New\"></img>\n",
        " [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APob7Owl6hBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "8a991300-b00e-4d39-fca3-acbf1ca12f78"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"USAID_support_functions.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1kHiQFmSf-bxsCb03yKufMAOJhsEblhm-\n",
        "\n",
        "Copyright 2019, Pavel Eftimovski, All rights reserved.\n",
        "\"\"\"\n",
        "\n",
        "## Section: Data Preprocessing ##\n",
        "\n",
        "import pandas as pd\n",
        "import os \n",
        "import sys\n",
        "\n",
        "#SCMS Data:\n",
        "\n",
        "# generate blocks of columns from same data type, count rows with null values\n",
        "def sort_by_type(dataset):\n",
        "    \n",
        "    new_column_names = ['id', 'p_code',\n",
        "                        'pq', 'po_so',\n",
        "                        'asn_dn',\n",
        "                        'country',\n",
        "                        'managed_by',\n",
        "                        'flf_via',\n",
        "                        'vendor_terms',\n",
        "                        'ship_mode',\n",
        "                        'pq_client_date',\n",
        "                        'po_vendor_date',\n",
        "                        'sch_del_date',\n",
        "                        'del_date',\n",
        "                        'rec_del_date',\n",
        "                        'product_grp',\n",
        "                        'sub_class',\n",
        "                        'vendor',\n",
        "                        'itm_desc',\n",
        "                        'mol_test',\n",
        "                        'brand',\n",
        "                        'dosage',\n",
        "                        'dos_form',\n",
        "                        'unit_msr',\n",
        "                        'ln_itm_qty',\n",
        "                        'ln_itm_val',\n",
        "                        'pack_price',\n",
        "                        'unit_price',\n",
        "                        'manu_site',\n",
        "                        'first_line',\n",
        "                        'weight',\n",
        "                        'freight_cost',\n",
        "                        'line_item']\n",
        "    \n",
        "    dataset = pd.read_excel(dataset)\n",
        "    newcol_dict = dict(zip(dataset.columns, new_column_names))\n",
        "    dataset.rename(columns=(newcol_dict), inplace =True)\n",
        "    blocks = dataset.as_blocks()\n",
        "    \n",
        "    for key in blocks.keys():\n",
        "        print(\"Type: {} , Count: {} \\nColumn name and null counts: \\n{}\\n\".format(\n",
        "            key,len(blocks[key].columns),blocks[key].isnull().sum()))\n",
        "        \n",
        "    return blocks\n",
        "\n",
        "\n",
        "# replace null/nan values\n",
        "def replace_nan(data, column, fillvalue):\n",
        "    print(\"-Before: \\n\", data.isnull().sum()[data.isnull().sum()>0])\n",
        "    # use 'TestKit/Ancillary' as dosage value\n",
        "    data[column].fillna(value=fillvalue, inplace=True)\n",
        "    # re-check null values\n",
        "    print(\"\\n-After: \\n\", data.isnull().sum()[data.isnull().sum()>0])\n",
        "    return data\n",
        "\n",
        "\n",
        "# describes the columns with null/nan values\n",
        "def describe_nulls(data, column):\n",
        "    nulls = data[data[column].isnull()]\n",
        "    for col in nulls:\n",
        "        print('\\n-------\\n',nulls[col].describe())\n",
        "\n",
        "        \n",
        "# otuput a summary of columns that correspond to null column\n",
        "def nan_desc(data, column):\n",
        "    is_null = data[data[column].isnull()]\n",
        "    for col in is_null:\n",
        "        print('\\n-------\\n',is_null[col].describe())\n",
        "\n",
        "        \n",
        "# LPI Data: rank countries by score in missing years\n",
        "def rank_missing(df_countries, missing_col):\n",
        "    for row in missing_col:\n",
        "      year, suffix = row.split(':')[0], row.split(':')[1]\n",
        "      col_base = suffix.split('_')[0]\n",
        "      if '_rank' in row:\n",
        "         df_countries[row]=df_countries[year+':'+col_base+'_score'].rank(ascending=0)\n",
        "\n",
        "            \n",
        "# Locate manufacturing site with Google Maps API:\n",
        "import urllib\n",
        "import requests\n",
        "\n",
        "main_p = '.../USAID_Data/' \n",
        "ctry_cont = pd.read_csv(main_p+\"country code_to continent.csv\") # country location in continents\n",
        "ctry_cont_dict = {x:y for x,y in zip(ctry_cont['Country Code'],ctry_cont['Continent'])}\n",
        "    \n",
        "def locateSite(manu_site, ctry_cont_dict = ctry_cont_dict):\n",
        "    \n",
        "    google_api = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
        "    API_KEY = '...' \n",
        "    url = google_api + urllib.parse.urlencode({'address': manu_site}) # set the location's url on the map    \n",
        "    \n",
        "    json = requests.get(url, params={\"key\": API_KEY}).json()# get data in .json format    \n",
        "    \n",
        "    f_address = json['results'][0]['formatted_address'] # extract formatted adress\n",
        "    items_bool = [x['types'] == ['country', 'political'] for x in json['results'][0]['address_components']]\n",
        "    i = items_bool.index(True)\n",
        "    ctry_code = json['results'][0]['address_components'][i]['short_name'] # get short name of country\n",
        "    ctry = json['results'][0]['address_components'][i]['long_name'] # get long name of country\n",
        "    cnt= ctry_cont_dict[ctry_code] #get continent\n",
        "    return f_address, ctry, cnt\n",
        "\n",
        "\n",
        "# compare contries from two data frames\n",
        "def compare_countries(df1,df2,ctry1,ctry2):\n",
        "    \n",
        "    fsi_ser, lpi_ser = pd.Series(df1[ctry1].unique(), name='fsi'),pd.Series(df2[ctry2].unique(), name='lpi')\n",
        "    print(\"df1 shape:\",len(fsi_ser),\"df2 shape:\",len(lpi_ser))\n",
        "    \n",
        "    comparison_df = pd.merge(pd.DataFrame(fsi_ser),pd.DataFrame(lpi_ser), left_on='fsi', right_on='lpi'\n",
        "             , suffixes=('_fsi', '_lsi'), how='outer')\n",
        "    \n",
        "    # output comparison\n",
        "    df1_not_df2 = list(comparison_df[comparison_df.fsi.isnull()].lpi)\n",
        "    df2_not_df1 = list(comparison_df[comparison_df.lpi.isnull()].fsi)\n",
        "    print(\"In df1, not in df2 length: {} \\nIn df2, not in df1 length: {}\".format(len(df1_not_df2), len(df2_not_df1)))\n",
        "    print(\"In df1, not in df2: {} \\nIn df2, not in df1: {}\".format(df1_not_df2, df2_not_df1))\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "# split df to features\n",
        "def split_to_features(t_data, base_exam, feat_names):\n",
        "    features_dict = {}\n",
        "    temp = 0\n",
        "    for i in range(len(feat_names)):\n",
        "        if 'item_desc' in feat_names[i]: \n",
        "            features_dict[feat_names[i]] = t_data.iloc[:, :len(base_exam[i].columns)]\n",
        "        else:\n",
        "            features_dict[feat_names[i]] = t_data.iloc[:, \n",
        "            len(base_exam[i-1].columns)+temp:len(base_exam[i-1].columns)+len(base_exam[i].columns)+temp]\n",
        "            temp += len(base_exam[i-1].columns)\n",
        "    return features_dict\n",
        "\n",
        "## Section: Data Exploration ##\n",
        "\n",
        "# import model data\n",
        "def import_sel_data(files_path, feat_names):\n",
        "    \n",
        "    file_names = [feat_names[i]+\".csv\" for i in range(len(feat_names))]\n",
        "    file_dict = {f: None for f in feat_names}\n",
        "\n",
        "    for f in range(len(file_names)):\n",
        "        file_dict[feat_names[f]] = pd.read_csv(files_path + file_names[f])\n",
        "    \n",
        "    for file in file_dict.values():\n",
        "        try:\n",
        "            file.drop('Unnamed: 0', axis=1, inplace=True) # drop unnamed column\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return file_dict\n",
        "\n",
        "\n",
        "# one-hot-encoder\n",
        "from sklearn.pipeline import TransformerMixin\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "class OneHotEncoder(TransformerMixin):\n",
        "    \n",
        "    def __init__(self, cat=True):\n",
        "        self.cat = cat\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        self._validate = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        self._validate = True\n",
        "        return pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    \n",
        "# labeler\n",
        "class LabelBiner(TransformerMixin):\n",
        "    \n",
        "    def __init__(self, lab=True):\n",
        "        self.lab = lab\n",
        "    \n",
        "    def fit(self, y, X=None):\n",
        "        self._validate = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, y):\n",
        "        self._validate = True\n",
        "        enc = LabelBinarizer()\n",
        "        return enc.fit_transform(y)\n",
        "\n",
        "    \n",
        "# output PCA results\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def describe_pca(df, pca):\n",
        "\n",
        "    # components\n",
        "    dims = ['Component {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
        "    comps = pd.DataFrame(np.round(pca.components_, 4), columns = df.keys())\n",
        "    comps.index = dims\n",
        "    \n",
        "    # variance\n",
        "    r = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
        "    var = pd.DataFrame(np.round(r, 4), columns = ['Variance'])\n",
        "    var.index = dims\n",
        "    \n",
        "    # visualize\n",
        "    fig, ((ax0,ax1),(ax2,ax3)) = plt.subplots(\n",
        "        nrows=2, ncols=2, figsize=(14,10))\n",
        "    axes = [ax0,ax1,ax2,ax3]\n",
        "    \n",
        "    # plot\n",
        "    for dim in dims:\n",
        "        i = dims.index(dim)\n",
        "        idx = abs(comps.loc[dim,:])>0.1 # select features with > 10% contribution\n",
        "        comps.loc[dim,:][idx].sort_values(ascending=False).plot(\n",
        "                                                    kind=\"barh\",\n",
        "                                                    ax=axes[i],\n",
        "                                                    title=dim)\n",
        "        axes[i].set_xlabel(\"Feature Contribution\")\n",
        "\n",
        "    pd.concat([var, comps], axis = 1)\n",
        "\n",
        "    \n",
        "# fill nulls\n",
        "def fix_nulls(df):\n",
        "    df_nulls = df.isnull().sum()[df.isnull().sum()>0].index.tolist()\n",
        "    for c in df_nulls:\n",
        "        try:\n",
        "            cmean = df[c].mean()\n",
        "            df[c].fillna(cmean, inplace=True)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "## Section: ML Model Building ##\n",
        "\n",
        "# categorical data pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def cat_pipeline(s_objects, s_ts_objects):\n",
        "    s_objects['train'] = 1\n",
        "    s_ts_objects['train'] = 0\n",
        "    temp_objects = pd.concat([s_objects, s_ts_objects])\n",
        "\n",
        "    one_hot_pipeline = make_pipeline(OneHotEncoder())\n",
        "    temp_objects = one_hot_pipeline.fit_transform(temp_objects)\n",
        "\n",
        "    s_objects = temp_objects.loc[temp_objects['train'] == 1]\n",
        "    s_ts_objects = temp_objects.loc[temp_objects['train'] == 0]\n",
        "    s_objects.drop(['train'], axis=1, inplace=True)\n",
        "    s_ts_objects.drop(['train'], axis=1, inplace=True)\n",
        "    \n",
        "    return s_objects, s_ts_objects\n",
        "\n",
        "\n",
        "# numerical data pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "\n",
        "def transform_pipepline(t_objects, s_num_log, si, sc, sv, sm, sb, sml, trending, del_del):\n",
        "\n",
        "    log_transform = FunctionTransformer(np.log1p)\n",
        "    log_stand_pipeline = make_pipeline(log_transform, StandardScaler()) \n",
        "    stand_pipeline = make_pipeline(StandardScaler()) \n",
        "    lab_pipeline = make_pipeline(LabelBiner())\n",
        "\n",
        "    ti, tc, tv, tm, tb, tml = [pd.DataFrame(log_stand_pipeline.fit_transform(d)\n",
        "                                            , index=d.index\n",
        "                                            , columns=d.columns) for d in [si, sc, sv, sm, sb, sml]]\n",
        "    \n",
        "    t_num_log = pd.DataFrame(stand_pipeline.fit_transform(s_num_log)\n",
        "                                             , index=s_num_log.index\n",
        "                                             , columns=s_num_log.columns)\n",
        "\n",
        "    tt = pd.DataFrame(stand_pipeline.fit_transform(trending)\n",
        "                                             , index=trending.index\n",
        "                                             , columns=trending.columns)\n",
        "    \n",
        "    \n",
        "    delayed_del = lab_pipeline.fit_transform(del_del.delayed_del.map({True:1, False:0}))\n",
        "    delayed_del = pd.DataFrame(delayed_del)\n",
        "    delayed_del.rename(columns = {0: 'delayed_del'}, inplace = True)\n",
        "\n",
        "    t_data = pd.concat([t_objects, t_num_log\n",
        "                        , ti, tc, tv, tm, tb\n",
        "                        , tml, tt], axis=1)\n",
        "    \n",
        "    print(\"X shape: \", t_data.shape, \"\\ny shape:\", delayed_del.shape)\n",
        "    \n",
        "    return t_data, delayed_del\n",
        "    \n",
        "    \n",
        "# classifier: split to train and test datasets\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_set(X, y, ts, use_smote=False):\n",
        "  \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=ts,\n",
        "                                                        random_state=103)\n",
        "    smote = SMOTE(ratio = 1.0, random_state=103)    \n",
        "    \n",
        "    if use_smote:         \n",
        "        X_train, y_train = smote.fit_sample(X_train, y_train)\n",
        "    \n",
        "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "    y_train = pd.DataFrame(y_train, columns=y.columns)\n",
        "    \n",
        "    print(\"Shape of X_train: {} X_val: {} y_train: {} y_val: {}\".format(X_train.shape,\n",
        "                                                                        X_test.shape,\n",
        "                                                                        y_train.shape,\n",
        "                                                                        y_test.shape))  \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# regression: split dataset\n",
        "def split_set_reg(X, true_pred, datetime, delayed_del):\n",
        "\n",
        "    # split for training\n",
        "    X_train_total = X.loc[delayed_del[delayed_del==1].dropna().index.tolist(),:]\n",
        "    X_train = X_train_total.drop(true_pred.index.tolist(), axis=0)\n",
        "\n",
        "    y_train_total = datetime.loc[delayed_del[delayed_del==1].dropna().index.tolist(),['delay_t']]\n",
        "    y_ttemp = y_train_total.drop(true_pred.index.tolist(), axis=0)\n",
        "    y_train = y_ttemp.delay_t.dt.days\n",
        "\n",
        "    # split for testing\n",
        "    X_test = X.loc[true_pred.index.tolist(),:]\n",
        "    y_test = datetime.loc[true_pred.index.tolist(),['delay_t']]['delay_t'].dt.days\n",
        "    \n",
        "    print(\"Shape of X_train: {} X_val: {} y_train: {} y_val: {}\".format(X_train.shape,\n",
        "                                                                        X_test.shape,\n",
        "                                                                        y_train.shape,\n",
        "                                                                        y_test.shape))\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# select classifier\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score, r2_score, mean_squared_error\n",
        "\n",
        "def select_classifier(X_train, X_test, y_train, y_test, clf):\n",
        "\n",
        "    ml = Pipeline([('estimator', clf)])\n",
        "\n",
        "    y_train = preprocessing.LabelEncoder().fit_transform(y_train.values.ravel())\n",
        "    y_test = preprocessing.LabelEncoder().fit_transform(y_test.values.ravel())\n",
        "    \n",
        "    ml.fit(X_train, y_train) # fit\n",
        "    pred = ml.predict(X_test) # predict\n",
        "    \n",
        "    return (f1_score(y_test, pred)) # return f1 score\n",
        "\n",
        "\n",
        "# collect true positivis from clf\n",
        "def collect_tr_pred(sel_clf, X_train, X_test, y_train, y_test):\n",
        "\n",
        "    sel_clf.fit(X_train, y_train) # fit\n",
        "    y_pred = sel_clf.predict(X_test) # predict\n",
        "    \n",
        "    pred_df = y_test.copy() \n",
        "    pred_df['prediction']= y_pred\n",
        "\n",
        "    true_pred_df = pred_df[(pred_df.delayed_del==1) & (pred_df.delayed_del==pred_df.prediction)]\n",
        "    print(\"True positives:\\n\", true_pred_df.sum())\n",
        "    return true_pred_df\n",
        "\n",
        "\n",
        "# select regressor\n",
        "def select_regressor(X_train, X_test, y_train, y_test, reg):\n",
        "\n",
        "    ml = Pipeline([('estimator', reg)]) \n",
        "    \n",
        "    y_train = y_train.values.ravel()\n",
        "    y_test = y_test.values.ravel()\n",
        "    \n",
        "    ml.fit(X_train, y_train) # fit\n",
        "    pred = ml.predict(X_test) # predict\n",
        "    \n",
        "    r_score = (r2_score(y_test, pred), np.sqrt(mean_squared_error(y_test, pred)))\n",
        "    return r_score # return r2, rmse\n",
        "\n",
        "\n",
        "# automate classification and regression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def classify_regress(X, y, datetime, clf, reg, test_size=0.2, use_smote=False):\n",
        "    \n",
        "    # Classification\n",
        "    X_train_c, X_test_c, y_train_c, y_test_c = split_set(X, y, test_size, use_smote=True)\n",
        "    clf_fit = clf.fit(X_train_c, y_train_c)\n",
        "    \n",
        "    # feature importance\n",
        "    try:\n",
        "        clf_importance = visualize_importance(clf_fit, X_train_c, 30)\n",
        "    except AttributeError:\n",
        "        clf_importance = pd.DataFrame([[]])\n",
        "        print(\"\\nCan't visualize feature importance for:\", clf)\n",
        "        \n",
        "    # classification report, conf matrix  \n",
        "    clf_report, conf_mtrx=[rm(y_test_c, clf_fit.predict(X_test_c)) for rm in [classification_report,\n",
        "                                                                                  confusion_matrix]]\n",
        "    \n",
        "    # true predicted \n",
        "    predicted = y_test_c.copy()\n",
        "    predicted['prediction']= clf.predict(X_test_c)\n",
        "    true_pred = predicted[(predicted.delayed_del==1) & (predicted.delayed_del==predicted.prediction)]\n",
        "    \n",
        "    # Regression\n",
        "    X_train_r, X_test_r, y_train_r, y_test_r = split_set_reg(X, true_pred,\n",
        "                                                             datetime, y) \n",
        "    reg.fit(X_train_r,y_train_r)\n",
        "    \n",
        "    # reg scores\n",
        "    r2 = r2_score(y_test_r, reg.predict(X_test_r))\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_r, reg.predict(X_test_r)))\n",
        "    \n",
        "    return predicted, true_pred, clf_importance, clf_report, conf_mtrx, r2, rmse  \n",
        "\n",
        "\n",
        "# plot feature importance\n",
        "def visualize_importance(model, df, xlim):\n",
        "    \n",
        "    features = df.columns\n",
        "    major_features = pd.DataFrame(model.feature_importances_, features)\n",
        "    major_features.reset_index(inplace=True)\n",
        "    \n",
        "    major_features.columns=['feature', 'importance']\n",
        "    major_features.set_index('feature').sort_values(\n",
        "                                        'importance', ascending=False)[:30].plot(\n",
        "                                                                        kind=\"barh\",\n",
        "                                                                        xlim=(0, xlim),\n",
        "                                                                        figsize=(10,7),\n",
        "                                                                        color='tab:blue')\n",
        "    \n",
        "    \n",
        "# classification report\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "def class_report(X_train, X_test, y_train, y_test, est):    \n",
        "    ml = Pipeline([('estimator', est)])\n",
        "    report = ClassificationReport(ml, classes=['on-time', 'delayed']\n",
        "                                                    , cmap='YlGnBu')\n",
        "    report.fit(X_train, y_train)\n",
        "    report.score(X_test, y_test)\n",
        "    return report.poof()\n",
        "\n",
        "\n",
        "# cross-validation score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "import seaborn as sns\n",
        "\n",
        "def cross_val_report(clf, X, y, report_name):\n",
        "    \n",
        "    y = y.values.ravel()\n",
        "    scores = cross_validate(clf, X, y, cv=5, scoring=('precision', 'recall', 'f1'))\n",
        "    \n",
        "    validation_scores = {'precision': {}, 'recall': {}, 'f1': {}}\n",
        "    \n",
        "    for keys, values in scores.items():\n",
        "        \n",
        "            if 'precision' in keys:\n",
        "                if 'train' in keys:\n",
        "                    validation_scores['precision']['on-time'] = values.mean()\n",
        "                elif 'test' in keys:\n",
        "                    validation_scores['precision']['delayed'] = values.mean()\n",
        "                    \n",
        "            elif 'recall' in keys:\n",
        "                if 'train' in keys:\n",
        "                    validation_scores['recall']['on-time'] = values.mean()\n",
        "                elif 'test' in keys:\n",
        "                    validation_scores['recall']['delayed'] = values.mean()\n",
        "                    \n",
        "            elif 'f1' in keys:\n",
        "                if 'train' in keys:\n",
        "                    validation_scores['f1']['on-time'] = values.mean()\n",
        "                elif 'test' in keys:\n",
        "                    validation_scores['f1']['delayed'] = values.mean()        \n",
        "\n",
        "    a, b = plt.subplots(1, 1, figsize=(6,5))\n",
        "    ax = plt.axes()\n",
        "    ax.set_title(report_name)\n",
        "    return sns.heatmap(pd.DataFrame.from_dict(validation_scores),\n",
        "                                                       ax = ax,\n",
        "                                                       annot=True,\n",
        "                                                       fmt='.3f',\n",
        "                                                       linewidths=.5,\n",
        "                                                       cmap=\"YlGnBu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f57d2fb117bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mmain_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.../USAID_Data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mctry_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_p\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"country code_to continent.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# country location in continents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mctry_cont_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctry_cont\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctry_cont\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Continent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'.../USAID_Data/country code_to continent.csv' does not exist: b'.../USAID_Data/country code_to continent.csv'"
          ]
        }
      ]
    }
  ]
}